# -*- coding: utf-8 -*-
"""student performance predictions .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RHBerCkw3zeIvf8pOV4kDfGq0mPnnVrf

1) Problem statement

    This project understands how the student's performance (test scores) is affected by other variables such as Gender, Ethnicity, Parental level of education, Lunch and Test preparation course.

2) Data Collection

    Dataset Source - https://www.kaggle.com/datasets/spscientist/students-performance-in-exams?datasetId=74977
    The data consists of 8 column and 1000 rows.
"""

from google.colab import drive
drive.mount('/content/drive')

"""importing all the necessary libraries necessary for performing EDA"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv(r"/content/drive/My Drive/stud.csv")

## we can see there are 1000 rows and 8 columns
df

df.head(5)

df.shape

"""##undersatnding about the data

    gender : sex of students -> (Male/female)
    race/ethnicity : ethnicity of students -> (Group A, B,C, D,E)
    parental level of education : parents' final education ->(bachelor's degree,some college,master's degree,associate's degree,high school)
    lunch : having lunch before test (standard or free/reduced)
    test preparation course : complete or not complete before test
    math score
    reading score
    writing score


"""

## checking missing values

df.isna().sum()

##checking duplicates
df.duplicated().sum()

# checking the datatypes
df.dtypes

## checking unique values of each data
df.nunique()

## checking stastics
df.describe()

"""Insight

    From above description of numerical data, all means are very close to each other - between 66 and 68.05;
    All standard deviations are also close - between 14.6 and 15.19;
    While there is a minimum score 0 for math, for writing minimum is much higher = 10 and for reading minimum higher = 17

exploratory data
"""

print("Categories in 'gender' variable:     ",end=" " )
print(df['gender'].unique())

print("Categories in 'race_ethnicity' variable:  ",end=" ")
print(df['race_ethnicity'].unique())

print("Categories in'parental level of education' variable:",end=" " )
print(df['parental_level_of_education'].unique())

print("Categories in 'lunch' variable:     ",end=" " )
print(df['lunch'].unique())

print("Categories in 'test preparation course' variable:     ",end=" " )
print(df['test_preparation_course'].unique())

# define numerical & categorical columns
numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O']
categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O']

# print columns
print('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))
print('\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))

"""## adding column for total score and average"""

df['total score'] = df['math_score'] + df['reading_score'] + df['writing_score']
df['average'] = df['total score']/3
df.head()

reading_full = df[df['reading_score'] == 100]['average'].count()
writing_full = df[df['writing_score'] == 100]['average'].count()
math_full = df[df['math_score'] == 100]['average'].count()

print(f'Number of students with full marks in Maths: {math_full}')
print(f'Number of students with full marks in Writing: {writing_full}')
print(f'Number of students with full marks in Reading: {reading_full}')

reading_less_20 = df[df['reading_score'] <= 20]['average'].count()
writing_less_20 = df[df['writing_score'] <= 20]['average'].count()
math_less_20 = df[df['math_score'] <= 20]['average'].count()

print(f'Number of students with less than 20 marks in Maths: {math_less_20}')
print(f'Number of students with less than 20 marks in Writing: {writing_less_20}')
print(f'Number of students with less than 20 marks in Reading: {reading_less_20}')

"""Insights

    From above values we get students have performed the worst in Maths
    Best performance is in reading section



---

4. Exploring Data ( Visualization )
4.1 Visualize average score distribution to make some conclusion.

    Histogram
    Kernel Distribution Function (KDE)

4.1.1 Histogram & KDE
"""

fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=df,x='average',bins=30,kde=True,color='g')
plt.subplot(122)
sns.histplot(data=df,x='average',kde=True,hue='gender')
plt.show()

fig, axs = plt.subplots(1, 2, figsize=(15, 7))
plt.subplot(121)
sns.histplot(data=df,x='total score',bins=30,kde=True,color='g')
plt.subplot(122)
sns.histplot(data=df,x='total score',kde=True,hue='gender')
plt.show()

"""
Insights

    Female students tend to perform well then male students.

"""

plt.subplots(1,3,figsize=(25,6))
plt.subplot(141)
sns.histplot(data=df,x='average',kde=True,hue='lunch')
plt.subplot(142)
sns.histplot(data=df[df.gender=='female'],x='average',kde=True,hue='lunch')
plt.subplot(143)
sns.histplot(data=df[df.gender=='male'],x='average',kde=True,hue='lunch')
plt.show()

"""
Insights

    Standard lunch helps perform well in exams.
    Standard lunch helps perform well in exams be it a male or a female.

"""

plt.subplots(1,3,figsize=(25,6))
plt.subplot(141)
ax =sns.histplot(data=df,x='average',kde=True,hue='parental_level_of_education')
plt.subplot(142)
ax =sns.histplot(data=df[df.gender=='male'],x='average',kde=True,hue='parental_level_of_education')
plt.subplot(143)
ax =sns.histplot(data=df[df.gender=='female'],x='average',kde=True,hue='parental_level_of_education')
plt.show()

"""
Insights

    In general parent's education don't help student perform well in exam.
    2nd plot shows that parent's whose education is of associate's degree or master's degree their male child tend to perform well in exam
    3rd plot we can see there is no effect of parent's education on female students.

"""

plt.subplots(1,3,figsize=(25,6))
plt.subplot(141)
ax =sns.histplot(data=df,x='average',kde=True,hue='race_ethnicity')
plt.subplot(142)
ax =sns.histplot(data=df[df.gender=='female'],x='average',kde=True,hue='race_ethnicity')
plt.subplot(143)
ax =sns.histplot(data=df[df.gender=='male'],x='average',kde=True,hue='race_ethnicity')
plt.show()

"""Insights

    Students of group A and group B tends to perform poorly in exam.
    Students of group A and group B tends to perform poorly in exam irrespective of whether they are male or female

maximum score of students in all three subject
"""

plt.figure(figsize=(18,8))
plt.subplot(1, 4, 1)
plt.title('MATH SCORES')
sns.violinplot(y='math_score',data=df,color='red',linewidth=3)
plt.subplot(1, 4, 2)
plt.title('READING SCORES')
sns.violinplot(y='reading_score',data=df,color='green',linewidth=3)
plt.subplot(1, 4, 3)
plt.title('WRITING SCORES')
sns.violinplot(y='writing_score',data=df,color='blue',linewidth=3)
plt.show()

"""Insights

    From the above three plots its clearly visible that most of the students score in between 60-80 in Maths whereas in reading and writing most of them score from 50-80

multivariate analysis using pieplot
"""

plt.rcParams['figure.figsize'] = (30, 12)

plt.subplot(1, 5, 1)
size = df['gender'].value_counts()
labels = 'Female', 'Male'
color = ['red','green']


plt.pie(size, colors = color, labels = labels,autopct = '.%2f%%')
plt.title('Gender', fontsize = 20)
plt.axis('off')



plt.subplot(1, 5, 2)
size = df['race_ethnicity'].value_counts()
labels = 'Group C', 'Group D','Group B','Group E','Group A'
color = ['red', 'green', 'blue', 'cyan','orange']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Race/Ethnicity', fontsize = 20)
plt.axis('off')



plt.subplot(1, 5, 3)
size = df['lunch'].value_counts()
labels = 'Standard', 'Free'
color = ['red','green']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Lunch', fontsize = 20)
plt.axis('off')


plt.subplot(1, 5, 4)
size = df['test_preparation_course'].value_counts()
labels = 'None', 'Completed'
color = ['red','green']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Test_Course', fontsize = 20)
plt.axis('off')


plt.subplot(1, 5, 5)
size = df['parental_level_of_education'].value_counts()
labels = 'Some College', "Associate's Degree",'High School','Some High School',"Bachelor's Degree","Master's Degree"
color = ['red', 'green', 'blue', 'cyan','orange','grey']

plt.pie(size, colors = color,labels = labels,autopct = '.%2f%%')
plt.title('Parental_Education', fontsize = 20)
plt.axis('off')


plt.tight_layout()
plt.grid()

plt.show()

"""Insights

    Number of Male and Female students is almost equal
    Number students are greatest in Group C
    Number of students who have standard lunch are greater
    Number of students who have not enrolled in any test preparation course is greater
    Number of students whose parental education is "Some College" is greater followed closely by "Associate's Degree"

feature wise visualization

gender column:

    How is distribution of Gender ?
    Is gender has any impact on student's performance ?

univariate analysis (how is distribution of gender?)
"""

f,ax=plt.subplots(1,2,figsize=(20,10))
sns.countplot(x=df['gender'],data=df,palette ='bright',ax=ax[0],saturation=0.95)
for container in ax[0].containers:
    ax[0].bar_label(container,color='black',size=20)

plt.pie(x=df['gender'].value_counts(),labels=['Male','Female'],explode=[0,0.1],autopct='%1.1f%%',shadow=True,colors=['#ff4d4d','#ff8000'])
plt.show()

"""Insights

    Gender has balanced data with female students are 518 (48%) and male students are 482 (52%)

univarient ANALYSIS (checking what is educational background of students parent? is parent education has any imp on students performance)
"""



plt.figure(figsize = (8,6))
plt.style.use('fivethirtyeight')
sns.countplot(df['parental_level_of_education'], palette = "Blues")
plt.title('comaparision of parental education', fontweight = 30, fontsize = 30)
plt.xlabel('count')
plt.ylabel('Degree')
plt.show()

"""insight:
 largest number of parents are from some college

BIVARIENT ANALYSIS  checkinfg is there any impact of parents eduacation on students performance
"""

plt.figure(figsize = (8,6))
plt.bar(df["parental_level_of_education"], df["total score"])
plt.xlabel('parentlevel of education')
plt.ylabel('total score')
plt.show()

"""insight:
the socre of students whose parent possess master degree and associate degree is higher

IS test preparation couurse has andy impact on students performance
"""

plt.figure(figsize=(12,6))
plt.subplot(2,2,1)
sns.barplot (x=df['test_preparation_course'], y=df['math_score'])
plt.subplot(2,2,2)
sns.barplot (x=df['test_preparation_course'], y=df['reading_score'])
plt.subplot(2,2,3)
sns.barplot (x=df['test_preparation_course'], y=df['writing_score'])

"""Insights:
students who have completed the test preparation course have scores higher in all tree categoreies

# checking outliers udisng boxplot
"""

plt.boxplot(df["math_score"])
plt.title("boxplot for mathscore")
plt.ylabel('mathscore')
plt.show()
plt.boxplot(df["reading_score"])
plt.title("boxplot for reading score")
plt.ylabel('reading score')
plt.show()
plt.boxplot(df["writing_score"])
plt.title("boxplot for writing score")
plt.ylabel('writing score')
plt.show()

sns.pairplot(df,hue = 'gender')
plt.show()

"""Insights

    From the above plot it is clear that all the scores increase linearly with each other.

Conclusions
    Females lead in pass percentage and also are top-scorers
    Student's Performance is not much related with test preparation course
    Finishing preparation course is benefitial.

**# MACHINE LEARNING (MODEL BUILDING)**
"""

!pip install catboost

### IMPORTING ALL NECESSARY LIBRARIES

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.model_selection import RandomizedSearchCV
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
import warnings

"""splitting X and Y variables

dropping total scoreand average and keeping the total score as the output variable for the ease of calculation
"""

df.drop(["total score", "average"], axis = 1, inplace = True)

df.head()

x= df.drop(columns = ["math_score"], axis = 1)

x.head()

y = df["math_score"]

y.head()

y.shape

## seperating ctegorical and numerical features

num_feature = x.select_dtypes(exclude = "object").columns
cat_features = x.select_dtypes(include = "object").columns

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

numeric_transformer = StandardScaler()
cat_transformer = OneHotEncoder()

preprocessor = ColumnTransformer([("OneHotEncoder", cat_transformer, cat_features),
                                  ("StandadScaler", numeric_transformer, num_feature),])

x = preprocessor.fit_transform(x)

x.shape

"""SPLITTING DATA INTO TRAIN TEST SPLIT

"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)
x_train.shape, x_test.shape

"""creating the evaluation function to test after model training"""

def evaluation_model(true, predicted):
  mae = mean_absolute_error(true, predicted)
  mse = mean_squared_error(true, predicted)
  rmse = np.sqrt(mean_squared_error(true, predicted))
  r2_square= r2_score(true,predicted)
  return mae, rmse,r2_square

models = {"linear regression" : LinearRegression(),
          "Lasso": Lasso(),
          'Ridge': Ridge(),
          "k_neighbore regression" : KNeighborsRegressor()
          ,"Decision_Tree" : DecisionTreeRegressor(),
          "Rndom Forest Regressor": RandomForestRegressor(),
          "XGBoosting regressor" : XGBRegressor(),
          "Catboosting regressor" : CatBoostRegressor(verbose = False),
          "Adaboost regressor" : AdaBoostRegressor()
          }

model_list = []
r2_list = []

for i in range(len(list(models))):
  model = list(models.values())[i]
  model.fit(x_train, y_train)

  y_train_pred = model.predict(x_train)
  y_test_pred = model.predict(x_test)

  ## evaluate the tran and test dataset

  model_train_mae, model_train_rmse, model_train_r2 = evaluation_model(y_train, y_train_pred)
  model_test_mae, model_test_rmse, model_test_r2 = evaluation_model(y_test, y_test_pred)

  print(list(models.keys())[i])
  model_list.append(list(models.keys())[i])

  print('Model performance for Training set')
  print("- Root Mean Squared Error: {:.4f}".format(model_train_rmse))
  print("- Mean Absolute Error: {:.4f}".format(model_train_mae))
  print("- R2 Score: {:.4f}".format(model_train_r2))

  print('----------------------------------')

  print('Model performance for Test set')
  print("- Root Mean Squared Error: {:.4f}".format(model_test_rmse))
  print("- Mean Absolute Error: {:.4f}".format(model_test_mae))
  print("- R2 Score: {:.4f}".format(model_test_r2))
  r2_list.append(model_test_r2)

  print('='*35)
  print('\n')

"""RESULTS"""

pd.DataFrame(list(zip(model_list, r2_list)), columns = ["model_name", "r2 score"])

"""## from above we can see Ridge model gives the best score

> Add blockquote

linear regresssion
"""

lin_model = Ridge(fit_intercept = True)
lin_model = lin_model.fit(x_train, y_train)
y_pred = lin_model.predict(x_test)
score = r2_score(y_test, y_pred)*100
print("Accuracy of the model is %.2f" %score)

"""plotting y_pred and y_test"""

plt.scatter(y_test, y_pred);
plt.xlabel("Actual")
plt.ylabel('Predicted')

"""Difference between actual and predicted values"""

pred_df = pd.DataFrame({'Actual Value' : y_test, "predicted value": y_pred, "Difference" : y_test - y_pred})
pred_df

